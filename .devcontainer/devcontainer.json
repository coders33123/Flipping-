from typing import Callable, Dict, Type
import torch
import torch.nn as nn

# -------------------------
# Custom Activations (define or import yours here)
# -------------------------

class SoftPlus2(nn.Module):
    """SoftPlus2: log(exp(x)+1) - log(2), zero-centered variant of Softplus."""
    def __init__(self) -> None:
        super().__init__()
        self.ssp = nn.Softplus()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.ssp(x) - torch.log(torch.tensor(2.0))


class SoftExponential(nn.Module):
    """SoftExponential activation with learnable alpha."""
    def __init__(self, alpha: float | None = None):
        super().__init__()
        self.alpha = nn.Parameter(torch.tensor(0.0 if alpha is None else alpha))
        self.alpha.requires_grad_(True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        a = self.alpha.item()
        if a == 0.0:
            return x
        elif a < 0.0:
            return -torch.log(1.0 - a * (x + a)) / a
        else:
            return (torch.exp(a * x) - 1.0) / a + a


# -------------------------
# Activation Registry System
# -------------------------

_ACTIVATION_REGISTRY: Dict[str, Callable[..., nn.Module]] = {
    "relu": nn.ReLU,
    "leakyrelu": nn.LeakyReLU,
    "prelu": nn.PReLU,
    "gelu": nn.GELU,
    "sigmoid": nn.Sigmoid,
    "tanh": nn.Tanh,
    "softplus": nn.Softplus,
    "softplus2": SoftPlus2,
    "softexp": SoftExponential,
    "swish": nn.SiLU,
}


def register_activation(name: str, cls: Callable[..., nn.Module]) -> None:
    """Register a new activation function."""
    _ACTIVATION_REGISTRY[name.lower()] = cls


def get_activation(name: str, *args, **kwargs) -> nn.Module:
    """Retrieve an activation function by name."""
    name = name.lower()
    if name not in _ACTIVATION_REGISTRY:
        raise ValueError(f"Activation '{name}' not found in registry.")
    return _ACTIVATION_REGISTRY[name](*args, **kwargs)


def list_available_activations() -> list[str]:
    """List all registered activation function names."""
    return sorted(_ACTIVATION_REGISTRY.keys())


def describe_activations() -> None:
    """Print all registered activations with class info."""
    for name, cls in _ACTIVATION_REGISTRY.items():
        print(f"{name}: {cls.__name__ if hasattr(cls, '__name__') else cls.__class__.__name__}")


# -------------------------
# Optional: Decorator for Registration
# -------------------------

def activation(name: str):
    def wrapper(cls):
        register_activation(name, cls)
        return cls
    return wrapper


# -------------------------
# Example Usage
# -------------------------

# Manual class + registration
class MyCrazyActivation(nn.Module):
    def forward(self, x):
        return torch.sin(x**2) * torch.exp(-x)

register_activation("crazy", MyCrazyActivation)

# Or use decorator
@activation("weird")
class WeirdActivation(nn.Module):
    def forward(self, x):
        return torch.cos(x) * x

# Test
if __name__ == "__main__":
    act1 = get_activation("softplus2")
    act2 = get_activation("softexp", alpha=0.1)
    act3 = get_activation("crazy")
    act4 = get_activation("weird")

    print(list_available_activations())
    describe_activations()
